{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Derun-Chai/Assignment1/blob/https%2Fgithub.com%2FAlocinYerv%2FTulip.git/Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Author:** Derun Chai\n",
        "\n",
        "**Description:**\n",
        "\n",
        "This notebook demonstrates the future price prediction for different stocks using recurrent neural networks in tensorflow. Recurrent neural networks with basic, LSTM or GRU cells are implemented.\n",
        "\n",
        "**Outline:**\n",
        "\n",
        "1. [Libraries and settings](#1-bullet)\n",
        "2. [Analyze data](#2-bullet)\n",
        "3. [Manipulate data](#3-bullet)\n",
        "4. [Model and validate data](#4-bullet)\n",
        "5. [Predictions](#5-bullet)\n",
        "\n",
        "**Reference:**  \n",
        "\n",
        "[LSTM_Stock_prediction-20170507 by BenF](https://www.kaggle.com/benjibb/lstm-stock-prediction-20170507/notebook)"
      ],
      "metadata": {
        "_uuid": "c8c2cca5ed3dcf9ff50ba2da4c9db56329bfc2cb",
        "_cell_guid": "182a3810-aab7-4d65-8ff7-0ad50183d769",
        "id": "XaD987rlZ8cx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Libraries and settings <a class=\"anchor\" id=\"1-bullet\"></a>"
      ],
      "metadata": {
        "_uuid": "8ec772a02eb88c3446fe0f689b2e1ee8ec9deb6b",
        "_cell_guid": "1426ca5e-ed0c-4766-ac74-cd1dd38c680c",
        "id": "ok7MsLcfZ8cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzbWLrI8kFhD",
        "outputId": "20b236eb-4db6-47f6-fa57-a36f3b97e9b9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow)\n",
            "  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.16,>=2.15.0 (from tensorflow)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.14.0\n",
            "    Uninstalling tensorflow-estimator-2.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.14.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.14.0\n",
            "    Uninstalling keras-2.14.0:\n",
            "      Successfully uninstalled keras-2.14.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.14.1\n",
            "    Uninstalling tensorboard-2.14.1:\n",
            "      Successfully uninstalled tensorboard-2.14.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.14.0\n",
            "    Uninstalling tensorflow-2.14.0:\n",
            "      Successfully uninstalled tensorflow-2.14.0\n",
            "Successfully installed keras-2.15.0 tensorboard-2.15.1 tensorflow-2.15.0 tensorflow-estimator-2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import sklearn\n",
        "import sklearn.preprocessing\n",
        "import datetime\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "\n",
        "# split data in 80%/10%/10% train/validation/test sets\n",
        "valid_set_size_percentage = 10\n",
        "test_set_size_percentage = 10\n",
        "\n",
        "#display parent directory and working directory\n",
        "print(os.path.dirname(os.getcwd())+':', os.listdir(os.path.dirname(os.getcwd())));\n",
        "print(os.getcwd()+':', os.listdir(os.getcwd()));\n"
      ],
      "metadata": {
        "_uuid": "78e6c0015907947ff5fade7ad76317633d27177a",
        "_cell_guid": "5a142d3f-96b5-4378-bb01-903367721e08",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3EdfiqcZ8cz",
        "outputId": "05e1e8eb-2346-4da8-9da8-fe713488d2df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "/: ['sys', 'usr', 'etc', 'libx32', 'opt', 'lib64', 'media', 'sbin', 'root', 'boot', 'mnt', 'var', 'bin', 'home', 'dev', 'run', 'lib32', 'proc', 'srv', 'tmp', 'lib', '.dockerenv', 'tools', 'datalab', 'content', 'python-apt', 'NGC-DL-CONTAINER-LICENSE', 'cuda-keyring_1.0-1_all.deb']\n",
            "/content: ['.config', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Analyze data <a class=\"anchor\" id=\"2-bullet\"></a>\n",
        "- load stock prices from prices-split-adjusted.csv\n",
        "- analyze data"
      ],
      "metadata": {
        "_uuid": "9422490f417bc1fcac8e6a0a392faa7451173dac",
        "_cell_guid": "5416ce8a-24ff-4903-a09b-b2941eae71d9",
        "id": "RoLmHlrnZ8c0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import all stock prices\n",
        "df = pd.read_csv(\"prices-split-adjusted.csv\", index_col = 0)\n",
        "df.info()\n",
        "df.head()\n",
        "\n",
        "# number of different stocks\n",
        "print('\\nnumber of different stocks: ', len(list(set(df.symbol))))\n",
        "print(list(set(df.symbol))[:10])"
      ],
      "metadata": {
        "_uuid": "aef1074dd4f86a9fe5a380c83994123ce56cce9d",
        "_cell_guid": "60dbc696-dda8-476e-8227-010a79df3aa2",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "eoEhQfEyZ8c0",
        "outputId": "775518c7-fdd2-40ba-c361-f34e8ca75903"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a181b338eae5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import all stock prices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prices-split-adjusted.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'prices-split-adjusted.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "_uuid": "c06e709a1bfecb963213943dc7ce8cd559780d9b",
        "_cell_guid": "962ea6e1-51ff-4bba-ad30-77ca5c765d59",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "Djk2om-yZ8c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "_uuid": "60ee56dd22397b9c545ea7dcc318f5c733d419ba",
        "_cell_guid": "d1fc9207-adcd-44f4-85a1-065d34fd82f2",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "c5lrjfx5Z8c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "_uuid": "ef59e62b988d2b0bfd00f834a7ecc17cb927dfc9",
        "_cell_guid": "8e1e0b13-c1da-4fe1-b971-bb2ebcad453b",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "hG4JAQoRZ8c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 5));\n",
        "plt.subplot(1,2,1);\n",
        "plt.plot(df[df.symbol == 'EQIX'].open.values, color='red', label='open')\n",
        "plt.plot(df[df.symbol == 'EQIX'].close.values, color='green', label='close')\n",
        "plt.plot(df[df.symbol == 'EQIX'].low.values, color='blue', label='low')\n",
        "plt.plot(df[df.symbol == 'EQIX'].high.values, color='black', label='high')\n",
        "plt.title('stock price')\n",
        "plt.xlabel('time [days]')\n",
        "plt.ylabel('price')\n",
        "plt.legend(loc='best')\n",
        "#plt.show()\n",
        "\n",
        "plt.subplot(1,2,2);\n",
        "plt.plot(df[df.symbol == 'EQIX'].volume.values, color='black', label='volume')\n",
        "plt.title('stock volume')\n",
        "plt.xlabel('time [days]')\n",
        "plt.ylabel('volume')\n",
        "plt.legend(loc='best');"
      ],
      "metadata": {
        "_uuid": "1235c99db35b562f09d6fb5721368bcab68e307d",
        "_cell_guid": "ce84999a-306f-4586-b8d6-dc3a0ab9ab67",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "4o4Nx1CgZ8c1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Manipulate data <a class=\"anchor\" id=\"3-bullet\"></a>\n",
        "- choose a specific stock\n",
        "- drop feature: volume\n",
        "- normalize stock data\n",
        "- create train, validation and test data sets"
      ],
      "metadata": {
        "_uuid": "777e047511205f929abf0867c7c8a2edee2531ca",
        "_cell_guid": "73ad0def-f162-42b9-a1d6-aac45be27e81",
        "id": "59ZBh8uLZ8c1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function for min-max normalization of stock\n",
        "def normalize_data(df):\n",
        "    min_max_scaler = sklearn.preprocessing.MinMaxScaler()\n",
        "    df['open'] = min_max_scaler.fit_transform(df.open.values.reshape(-1,1))\n",
        "    df['high'] = min_max_scaler.fit_transform(df.high.values.reshape(-1,1))\n",
        "    df['low'] = min_max_scaler.fit_transform(df.low.values.reshape(-1,1))\n",
        "    df['close'] = min_max_scaler.fit_transform(df['close'].values.reshape(-1,1))\n",
        "    return df\n",
        "\n",
        "# function to create train, validation, test data given stock data and sequence length\n",
        "def load_data(stock, seq_len):\n",
        "    data_raw = stock.values # convert to numpy array\n",
        "    data = []\n",
        "\n",
        "    # create all possible sequences of length seq_len\n",
        "    for index in range(len(data_raw) - seq_len):\n",
        "        data.append(data_raw[index: index + seq_len])\n",
        "\n",
        "    data = np.array(data);\n",
        "    valid_set_size = int(np.round(valid_set_size_percentage/100*data.shape[0]));\n",
        "    test_set_size = int(np.round(test_set_size_percentage/100*data.shape[0]));\n",
        "    train_set_size = data.shape[0] - (valid_set_size + test_set_size);\n",
        "\n",
        "    x_train = data[:train_set_size,:-1,:]\n",
        "    y_train = data[:train_set_size,-1,:]\n",
        "\n",
        "    x_valid = data[train_set_size:train_set_size+valid_set_size,:-1,:]\n",
        "    y_valid = data[train_set_size:train_set_size+valid_set_size,-1,:]\n",
        "\n",
        "    x_test = data[train_set_size+valid_set_size:,:-1,:]\n",
        "    y_test = data[train_set_size+valid_set_size:,-1,:]\n",
        "\n",
        "    return [x_train, y_train, x_valid, y_valid, x_test, y_test]\n",
        "\n",
        "# choose one stock\n",
        "df_stock = df[df.symbol == 'EQIX'].copy()\n",
        "df_stock.drop(['symbol'],1,inplace=True)\n",
        "df_stock.drop(['volume'],1,inplace=True)\n",
        "\n",
        "cols = list(df_stock.columns.values)\n",
        "print('df_stock.columns.values = ', cols)\n",
        "\n",
        "# normalize stock\n",
        "df_stock_norm = df_stock.copy()\n",
        "df_stock_norm = normalize_data(df_stock_norm)\n",
        "\n",
        "# create train, test data\n",
        "seq_len = 20 # choose sequence length\n",
        "x_train, y_train, x_valid, y_valid, x_test, y_test = load_data(df_stock_norm, seq_len)\n",
        "print('x_train.shape = ',x_train.shape)\n",
        "print('y_train.shape = ', y_train.shape)\n",
        "print('x_valid.shape = ',x_valid.shape)\n",
        "print('y_valid.shape = ', y_valid.shape)\n",
        "print('x_test.shape = ', x_test.shape)\n",
        "print('y_test.shape = ',y_test.shape)\n"
      ],
      "metadata": {
        "_uuid": "3333a7ef79ad756a4b0190e8aef2ed8b3624d7d8",
        "_cell_guid": "e577ab98-344e-4a8e-88c2-09a7d45834b9",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "FDb_cqVsZ8c1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 5));\n",
        "plt.plot(df_stock_norm.open.values, color='red', label='open')\n",
        "plt.plot(df_stock_norm.close.values, color='green', label='low')\n",
        "plt.plot(df_stock_norm.low.values, color='blue', label='low')\n",
        "plt.plot(df_stock_norm.high.values, color='black', label='high')\n",
        "#plt.plot(df_stock_norm.volume.values, color='gray', label='volume')\n",
        "plt.title('stock')\n",
        "plt.xlabel('time [days]')\n",
        "plt.ylabel('normalized price/volume')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "scrolled": true,
        "_uuid": "c5ce171e4160f5e0175e4e5ab8e242f181262440",
        "_cell_guid": "27e07a3b-89ea-48cc-bd86-023785634d1e",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "KaAwhkceZ8c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Model and validate data <a class=\"anchor\" id=\"4-bullet\"></a>\n",
        "- RNNs with basic, LSTM, GRU cells\n"
      ],
      "metadata": {
        "_uuid": "a495f69f3a265f419cd5aa0a4119a8b0392365fd",
        "_cell_guid": "4375362b-7a7c-4572-947c-91f46c4fd77c",
        "id": "xH_2pESaZ8c2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Basic Cell RNN in tensorflow\n",
        "\n",
        "# Function to calculate moving average\n",
        "def calculate_moving_average(data, window_size=5):\n",
        "    ma = np.zeros((data.shape[0] - window_size + 1, data.shape[1]))\n",
        "    for i in range(data.shape[1]):\n",
        "        ma[:, i] = np.convolve(data[:, i], np.ones(window_size) / window_size, mode='valid')\n",
        "    return ma\n",
        "\n",
        "# Calculate moving averages for y_train, y_valid, y_test\n",
        "window_size = 5\n",
        "y_train_ma = calculate_moving_average(y_train, window_size)\n",
        "y_valid_ma = calculate_moving_average(y_valid, window_size)\n",
        "y_test_ma = calculate_moving_average(y_test, window_size)\n",
        "\n",
        "# Adjust y_train, y_valid, and y_test to match the length of the moving averages\n",
        "y_train_adjusted = y_train[window_size-1:]\n",
        "y_valid_adjusted = y_valid[window_size-1:]\n",
        "y_test_adjusted = y_test[window_size-1:]\n",
        "\n",
        "# Calculate residuals\n",
        "y_train_residual = y_train_adjusted - y_train_ma\n",
        "y_valid_residual = y_valid_adjusted - y_valid_ma\n",
        "y_test_residual = y_test_adjusted - y_test_ma\n",
        "\n",
        "\n",
        "# Adjust x_train, x_valid, and x_test to align with the new length of your target variables\n",
        "x_train_adjusted = x_train[-y_train_residual.shape[0]:, :, :]\n",
        "x_valid_adjusted = x_valid[-y_valid_residual.shape[0]:, :, :]\n",
        "x_test_adjusted = x_test[-y_test_residual.shape[0]:, :, :]\n",
        "print(\"Adjusted Shapes:\")\n",
        "print(f\"x_train_adjusted: {x_train_adjusted.shape}, y_train_residual: {y_train_residual.shape}\")\n",
        "print(f\"x_valid_adjusted: {x_valid_adjusted.shape}, y_valid_residual: {y_valid_residual.shape}\")\n",
        "print(f\"x_test_adjusted: {x_test_adjusted.shape}, y_test_residual: {y_test_residual.shape}\")\n",
        "\n",
        "index_in_epoch = 0;\n",
        "perm_array  = np.arange(x_train.shape[0])\n",
        "np.random.shuffle(perm_array)\n",
        "\n",
        "# Function to get the next batch\n",
        "def get_next_batch(batch_size, x_data, y_data, perm_array):\n",
        "    global index_in_epoch\n",
        "    start = index_in_epoch\n",
        "    index_in_epoch += batch_size\n",
        "    if index_in_epoch >= x_data.shape[0]:  # Use >= to ensure index_in_epoch does not exceed the array size\n",
        "        np.random.shuffle(perm_array)  # Shuffle the permutation array\n",
        "        start = 0  # Start next epoch\n",
        "        index_in_epoch = batch_size\n",
        "    end = min(index_in_epoch, x_data.shape[0])  # Ensure end does not exceed the array size\n",
        "    return x_data[perm_array[start:end]], y_data[perm_array[start:end]]\n",
        "\n",
        "# Model parameters\n",
        "n_steps = seq_len - 1\n",
        "n_inputs = 4\n",
        "n_neurons = 300\n",
        "n_outputs = 4\n",
        "n_layers = 2\n",
        "learning_rate = 0.0025\n",
        "batch_size = 100\n",
        "n_epochs = 70\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.Sequential()\n",
        "for i in range(n_layers):\n",
        "    if i == n_layers - 1:\n",
        "        # Last layer does not return sequences\n",
        "        model.add(tf.keras.layers.SimpleRNN(n_neurons, return_sequences=False))\n",
        "    else:\n",
        "        model.add(tf.keras.layers.SimpleRNN(n_neurons, return_sequences=True))\n",
        "model.add(tf.keras.layers.Dense(n_outputs))\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate), loss='mean_squared_error')\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=n_epochs, batch_size=batch_size, validation_data=(x_valid, y_valid))\n",
        "\n",
        "# Training data preparation\n",
        "index_in_epoch = 0\n",
        "perm_array = np.arange(x_train_adjusted.shape[0])\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    np.random.shuffle(perm_array)  # Shuffle at the start of each epoch\n",
        "    index_in_epoch = 0  # Reset index_in_epoch at the start of each epoch\n",
        "    for iteration in range(x_train_adjusted.shape[0] // batch_size):\n",
        "        x_batch, y_batch = get_next_batch(batch_size, x_train_adjusted, y_train_residual, perm_array)\n",
        "        model.train_on_batch(x_batch, y_batch)\n",
        "\n",
        "        if iteration % (5 * x_train_adjusted.shape[0] // batch_size) == 0:\n",
        "            mse_train = model.evaluate(x_train_adjusted, y_train_residual, verbose=0)\n",
        "            mse_valid = model.evaluate(x_valid_adjusted, y_valid_residual, verbose=0)\n",
        "            print(f'{epoch:.2f} epochs: MSE train/valid = {mse_train:.6f}/{mse_valid:.6f}')\n",
        "\n",
        "# Predictions\n",
        "y_train_pred_residual = model.predict(x_train_adjusted)\n",
        "y_valid_pred_residual = model.predict(x_valid_adjusted)\n",
        "y_test_pred_residual = model.predict(x_test_adjusted)\n",
        "\n",
        "# Final predictions (basic model prediction + predicted residual)\n",
        "# Adjust the length of the predicted residuals if necessary\n",
        "y_train_final_pred = y_train_ma + y_train_pred_residual[:y_train_ma.shape[0], :]\n",
        "y_valid_final_pred = y_valid_ma + y_valid_pred_residual[:y_valid_ma.shape[0], :]\n",
        "y_test_final_pred = y_test_ma + y_test_pred_residual[:y_test_ma.shape[0], :]\n",
        "\n",
        "\n",
        "\n",
        "# use Basic LSTM Cell\n",
        "#layers = [tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons, activation=tf.nn.elu)\n",
        "#          for layer in range(n_layers)]\n",
        "\n",
        "# use LSTM Cell with peephole connections\n",
        "#layers = [tf.contrib.rnn.LSTMCell(num_units=n_neurons,\n",
        "#                                  activation=tf.nn.leaky_relu, use_peepholes = True)\n",
        "#          for layer in range(n_layers)]\n",
        "\n",
        "# use GRU cell\n",
        "#layers = [tf.contrib.rnn.GRUCell(num_units=n_neurons, activation=tf.nn.leaky_relu)\n",
        "#          for layer in range(n_layers)]\n",
        "\n",
        "\n",
        "# run graph\n",
        "# Extract the history of loss and validation loss\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "# Plotting the training and validation loss\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "_uuid": "b5015613883c52563da5f8e5be8d6adeb274e420",
        "_cell_guid": "dde96de3-fef3-45cf-959c-22171a28a3b6",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "7lDPdz-hZ8c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build the ARIMA model"
      ],
      "metadata": {
        "id": "Pb2Yc88rn924"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UKtxq_Lyn9Mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")  # Ignore convergence warnings for clarity\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "def check_stationarity(ts_data, title):\n",
        "    # Rolling statistics\n",
        "    roll_mean = ts_data.rolling(30).mean()\n",
        "    roll_std = ts_data.rolling(5).std()\n",
        "\n",
        "    # Plot rolling statistics\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(ts_data, color='black', label='Original Data')\n",
        "    plt.plot(roll_mean, color='red', label='Rolling Mean (30 days)')\n",
        "    plt.plot(roll_std, color='blue', label='Rolling Std Dev (5 days)')\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "    # Dickey-Fuller test\n",
        "    print(f'Dickey-Fuller Test: {title}\\n')\n",
        "    df_test = adfuller(ts_data, autolag='AIC')\n",
        "    test_result = pd.Series(df_test[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
        "    print(test_result)\n",
        "    for key, value in df_test[4].items():\n",
        "        print(f'Critical value at {key}: {value:.5f}')\n",
        "stocks = ['RIG', 'KMB', 'SWK', 'CTXS', 'GPN']\n",
        "\n",
        "for stock in stocks:\n",
        "    print(f\"\\nAnalyzing Stock: {stock}\\n{'='*30}\")\n",
        "    stock_data = df[df['symbol'] == stock]['close'].dropna()\n",
        "\n",
        "    # Checking stationarity\n",
        "    check_stationarity(stock_data, f'Stationarity Check for {stock}')\n",
        "\n",
        "    stock_data_log = np.log(stock_data)\n",
        "    stock_data_log_diff = stock_data_log - stock_data_log.shift()\n",
        "    stock_data_log_diff.dropna(inplace=True)\n",
        "    check_stationarity(stock_data_log_diff, f'Log Differenced {stock}')\n",
        "\n",
        "    # Fit ARIMA Model (example with ARIMA(1,1,0))\n",
        "    model = ARIMA(stock_data, order=(1, 1, 0))\n",
        "    model_fit = model.fit()\n",
        "    print(model_fit.summary())\n",
        "\n",
        "    # Forecasting (example)\n",
        "    forecast = model_fit.forecast(steps=5)\n",
        "    print(f\"\\nForecast for {stock}: {forecast}\\n\")\n"
      ],
      "metadata": {
        "id": "ByQvyIaDnwEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the model using LSTM layers"
      ],
      "metadata": {
        "id": "PguCCxCgdRi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Build the model using LSTM layers\n",
        "model_LSTM = tf.keras.Sequential()\n",
        "for i in range(n_layers - 1):\n",
        "    model_LSTM.add(tf.keras.layers.LSTM(n_neurons, return_sequences=True, activation='tanh', recurrent_activation='sigmoid'))\n",
        "model_LSTM.add(tf.keras.layers.LSTM(n_neurons, return_sequences=False, activation='tanh', recurrent_activation='sigmoid'))\n",
        "model_LSTM.add(tf.keras.layers.Dense(n_outputs))\n",
        "\n",
        "# Compile the model\n",
        "model_LSTM.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with adjusted data\n",
        "history_LSTM = model_LSTM.fit(x_train_adjusted, y_train_residual, epochs=n_epochs, batch_size=batch_size, validation_data=(x_valid_adjusted, y_valid_residual))\n",
        "\n",
        "# Make predictions using adjusted test set\n",
        "y_train_pred_residual_LSTM = model_LSTM.predict(x_train_adjusted)\n",
        "y_valid_pred_residual_LSTM = model_LSTM.predict(x_valid_adjusted)\n",
        "y_test_pred_residual_LSTM = model_LSTM.predict(x_test_adjusted)\n",
        "\n",
        "# Final predictions (basic model prediction + predicted residual)\n",
        "y_train_final_pred_LSTM = y_train_ma + y_train_pred_residual_LSTM[:y_train_ma.shape[0], :]\n",
        "y_valid_final_pred_LSTM = y_valid_ma + y_valid_pred_residual_LSTM[:y_valid_ma.shape[0], :]\n",
        "y_test_final_pred_LSTM = y_test_ma + y_test_pred_residual_LSTM[:y_test_ma.shape[0], :]\n",
        "\n",
        "# Extract the history of loss and validation loss\n",
        "loss = history_LSTM.history['loss']\n",
        "val_loss = history_LSTM.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "# Plotting the training and validation loss\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ak0HMwg5ZABb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_LSTM1 = tf.keras.Sequential([\n",
        "    # Add LSTM layers, note that input shape is required for the first layer.\n",
        "    tf.keras.layers.LSTM(n_neurons, return_sequences=True, input_shape=(19, n_inputs)),\n",
        "    tf.keras.layers.LSTM(n_neurons, return_sequences=True),\n",
        "    # You can only have return_sequences=False in the last LSTM layer\n",
        "    tf.keras.layers.LSTM(n_neurons, return_sequences=False),\n",
        "    # Output layer\n",
        "    tf.keras.layers.Dense(n_outputs)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_LSTM1.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model on the residuals with adjusted datasets\n",
        "history_LSTM1 = model_LSTM1.fit(x_train_adjusted, y_train_residual, epochs=n_epochs, batch_size=batch_size, validation_data=(x_valid_adjusted, y_valid_residual))\n",
        "\n",
        "# Make predictions (these are now residual predictions)\n",
        "y_train_pred_residual_LSTM1 = model_LSTM1.predict(x_train)\n",
        "y_valid_pred_residual_LSTM1 = model_LSTM1.predict(x_valid)\n",
        "y_test_pred_residual_LSTM1 = model_LSTM1.predict(x_test)\n",
        "# Adjust the length of predicted residuals to match the moving averages\n",
        "y_train_pred_residual_adjusted = y_train_pred_residual_LSTM1[-y_train_ma.shape[0]:]\n",
        "y_valid_pred_residual_adjusted = y_valid_pred_residual_LSTM1[-y_valid_ma.shape[0]:]\n",
        "y_test_pred_residual_adjusted = y_test_pred_residual_LSTM1[-y_test_ma.shape[0]:]\n",
        "\n",
        "# Calculate final predictions (basic model prediction + predicted residual)\n",
        "y_train_final_pred_LSTM1 = y_train_ma + y_train_pred_residual_adjusted\n",
        "y_valid_final_pred_LSTM1 = y_valid_ma + y_valid_pred_residual_adjusted\n",
        "y_test_final_pred_LSTM1 = y_test_ma + y_test_pred_residual_adjusted\n",
        "\n",
        "\n",
        "# Extract the history of loss and validation loss\n",
        "loss = history_LSTM1.history['loss']\n",
        "val_loss = history_LSTM1.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "# Plotting the training and validation loss\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XPY-SVDldE1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use GRU cell"
      ],
      "metadata": {
        "id": "L-WOUKN3f57t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_GRU = tf.keras.Sequential()\n",
        "for _ in range(n_layers - 1):\n",
        "    model_GRU.add(tf.keras.layers.GRU(n_neurons, return_sequences=True, activation='tanh'))\n",
        "model_GRU.add(tf.keras.layers.GRU(n_neurons, activation='tanh'))  # Last layer does not return sequences\n",
        "model_GRU.add(tf.keras.layers.Dense(n_outputs))\n",
        "\n",
        "model_GRU.compile(optimizer='adam', loss='mean_squared_error')\n",
        "# Trim x_train and x_valid to match the length of y_train_residual and y_valid_residual\n",
        "x_train_adjusted = x_train[-y_train_residual.shape[0]:]\n",
        "x_valid_adjusted = x_valid[-y_valid_residual.shape[0]:]\n",
        "\n",
        "# Verify that the shapes now match\n",
        "assert x_train_adjusted.shape[0] == y_train_residual.shape[0]\n",
        "assert x_valid_adjusted.shape[0] == y_valid_residual.shape[0]\n",
        "\n",
        "# Train the model on residuals\n",
        "\n",
        "history_GRU = model_GRU.fit(x_train_adjusted, y_train_residual, epochs=n_epochs, batch_size=batch_size, validation_data=(x_valid_adjusted, y_valid_residual))\n",
        "\n",
        "# Make predictions\n",
        "# Predict residuals\n",
        "\n",
        "y_train_pred_residual_GPU = model_GRU.predict(x_train_adjusted)\n",
        "y_valid_pred_residual_GPU = model_GRU.predict(x_valid_adjusted)\n",
        "y_test_pred_residual_GPU = model_GRU.predict(x_test)  # Assuming x_test is correctly adjusted\n",
        "# Trim the predicted residuals to match the length of the moving averages\n",
        "y_test_pred_residual_adjusted_GPU = y_test_pred_residual_GPU[-y_test_ma.shape[0]:]\n",
        "\n",
        "# Calculate final predictions (basic model prediction + predicted residual)\n",
        "y_train_final_pred_GPU = y_train_ma + y_train_pred_residual_GPU\n",
        "y_valid_final_pred_GPU = y_valid_ma + y_valid_pred_residual_GPU\n",
        "y_test_final_pred_GPU = y_test_ma + y_test_pred_residual_adjusted_GPU\n",
        "\n",
        "\n",
        "\n",
        "# run graph\n",
        "# Extract the history of loss and validation loss\n",
        "loss = history_GRU.history['loss']\n",
        "val_loss = history_GRU.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "# Plotting the training and validation loss\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K7VkZ-GKf5Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Predictions <a class=\"anchor\" id=\"5-bullet\"></a>"
      ],
      "metadata": {
        "_uuid": "a88583a9effb6f1f84e13b8853adca62578c2992",
        "_cell_guid": "9035de79-0f5c-4b6b-bc86-0d8d4e2f392e",
        "id": "gWcjGm_MZ8c3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "_uuid": "2923055320991790fe0c8c0ffa3e75558e5c313a",
        "_cell_guid": "412b33a3-1e02-4ed5-9846-9984022492bd",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "R51lE9OHZ8c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft = 0 # 0 = open, 1 = close, 2 = highest, 3 = lowest\n",
        "\n",
        "## show predictions\n",
        "plt.figure(figsize=(15, 5));\n",
        "plt.subplot(1,2,1);\n",
        "\n",
        "plt.plot(np.arange(y_train.shape[0]), y_train[:,ft], color='blue', label='train target')\n",
        "\n",
        "plt.plot(np.arange(y_train.shape[0], y_train.shape[0]+y_valid.shape[0]), y_valid[:,ft],\n",
        "         color='gray', label='valid target')\n",
        "\n",
        "plt.plot(np.arange(y_train.shape[0]+y_valid.shape[0],\n",
        "                   y_train.shape[0]+y_test.shape[0]+y_test.shape[0]),\n",
        "         y_test[:,ft], color='black', label='test target')\n",
        "\n",
        "plt.plot(np.arange(y_train_final_pred.shape[0]),y_train_final_pred[:,ft], color='red',\n",
        "         label='train prediction')\n",
        "\n",
        "plt.plot(np.arange(y_train_final_pred.shape[0], y_train_final_pred.shape[0]+y_valid_final_pred.shape[0]),\n",
        "         y_valid_final_pred[:,ft], color='orange', label='valid prediction')\n",
        "\n",
        "plt.plot(np.arange(y_train_final_pred.shape[0]+y_valid_final_pred.shape[0],\n",
        "                   y_train_final_pred.shape[0]+y_valid_final_pred.shape[0]+y_test_final_pred.shape[0]),\n",
        "         y_test_final_pred[:,ft], color='green', label='test prediction')\n",
        "\n",
        "plt.title('past and future stock prices')\n",
        "plt.xlabel('time [days]')\n",
        "plt.ylabel('normalized price')\n",
        "plt.legend(loc='best');\n",
        "\n",
        "plt.subplot(1,2,2);\n",
        "\n",
        "plt.plot(np.arange(y_train.shape[0], y_train.shape[0]+y_test.shape[0]),\n",
        "         y_test[:,ft], color='black', label='test target')\n",
        "\n",
        "plt.plot(np.arange(y_train_final_pred.shape[0], y_train_final_pred.shape[0]+y_test_final_pred.shape[0]),\n",
        "         y_test_final_pred[:,ft], color='green', label='test prediction')\n",
        "\n",
        "plt.title('future stock prices')\n",
        "plt.xlabel('time [days]')\n",
        "plt.ylabel('normalized price')\n",
        "plt.legend(loc='best');\n",
        "y_train_adjusted = y_train[-y_train_final_pred.shape[0]:]\n",
        "\n",
        "corr_price_development_train = np.sum(np.equal(np.sign(y_train_adjusted[:,1]-y_train_adjusted[:,0]),\n",
        "                                                np.sign(y_train_final_pred[:,1]-y_train_final_pred[:,0])).astype(int)) / y_train_adjusted.shape[0]\n",
        "y_valid_adjusted = y_valid[-y_valid_final_pred.shape[0]:]\n",
        "y_test_adjusted = y_test[-y_test_final_pred.shape[0]:]\n",
        "\n",
        "corr_price_development_valid = np.sum(np.equal(np.sign(y_valid_adjusted[:,1]-y_valid_adjusted[:,0]),\n",
        "                                               np.sign(y_valid_final_pred[:,1]-y_valid_final_pred[:,0])).astype(int)) / y_valid_adjusted.shape[0]\n",
        "\n",
        "corr_price_development_test = np.sum(np.equal(np.sign(y_test_adjusted[:,1]-y_test_adjusted[:,0]),\n",
        "                                              np.sign(y_test_final_pred[:,1]-y_test_final_pred[:,0])).astype(int)) / y_test_adjusted.shape[0]\n",
        "\n",
        "print('correct sign prediction for close - open price for train/valid/test: %.2f/%.2f/%.2f'%(\n",
        "    corr_price_development_train, corr_price_development_valid, corr_price_development_test))\n"
      ],
      "metadata": {
        "_uuid": "6fa5eb60c8e8a5d3542547629c3e0df14e5c33a3",
        "_cell_guid": "1bb898b9-943e-4e6f-b43d-8bdb6331cc4e",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "n9XQwUegZ8c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "collapsed": true,
        "_uuid": "2dc96fc968aa367482f1b7dc2018848af84bdb75",
        "_cell_guid": "581be505-fdc8-4aec-a3a3-19cd28f4e324",
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "P9K0plG8Z8c4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}